{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection of social bots using tweets\n",
    "\n",
    "\n",
    "#### Group 3: Stefani Platakidou, Kinga Jenei, Alejandro Lozada, Liam Le Tran\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line if you run the notebook for the first time\n",
    "\n",
    "#%pip install emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ignore print warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import emoji\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>language</th>\n",
       "      <th>geo_location</th>\n",
       "      <th>referenced_tweets_types</th>\n",
       "      <th>referenced_tweets_ids</th>\n",
       "      <th>user_mentions</th>\n",
       "      <th>media_types</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>299999</th>\n",
       "      <td>human</td>\n",
       "      <td>t1471504881882075140</td>\n",
       "      <td>17268418</td>\n",
       "      <td>2021-12-16 15:37:57+00:00</td>\n",
       "      <td>post</td>\n",
       "      <td>RT @ElemPE1: \"Toys for Tots\"üéÅ\\n\\nLess lecturin...</td>\n",
       "      <td>en</td>\n",
       "      <td>False</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{907432611319914497}</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300000</th>\n",
       "      <td>human</td>\n",
       "      <td>t1499064851347304449</td>\n",
       "      <td>33152005</td>\n",
       "      <td>2022-03-02 16:51:26+00:00</td>\n",
       "      <td>post</td>\n",
       "      <td>@DavidVimes Same.</td>\n",
       "      <td>en</td>\n",
       "      <td>False</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{966462024145391616}</td>\n",
       "      <td>{}</td>\n",
       "      <td>9.664620e+17</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300001</th>\n",
       "      <td>human</td>\n",
       "      <td>t1492208040795324425</td>\n",
       "      <td>76086369</td>\n",
       "      <td>2022-02-11 18:44:55+00:00</td>\n",
       "      <td>post</td>\n",
       "      <td>@BigelowLab I spy a @wellsreserve alumna!</td>\n",
       "      <td>en</td>\n",
       "      <td>False</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{260908660,76086369}</td>\n",
       "      <td>{}</td>\n",
       "      <td>2.609087e+08</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300002</th>\n",
       "      <td>human</td>\n",
       "      <td>t942173606888595457</td>\n",
       "      <td>709501413819293696</td>\n",
       "      <td>2017-12-16 23:24:27+00:00</td>\n",
       "      <td>post</td>\n",
       "      <td>RT @MPSHaringey: Who thinks #PDBoots deserves ...</td>\n",
       "      <td>en</td>\n",
       "      <td>False</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{407149465}</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300003</th>\n",
       "      <td>human</td>\n",
       "      <td>t1498058873269370880</td>\n",
       "      <td>2218413296</td>\n",
       "      <td>2022-02-27 22:14:02+00:00</td>\n",
       "      <td>post</td>\n",
       "      <td>1 BTC = 37,535 USD</td>\n",
       "      <td>vi</td>\n",
       "      <td>False</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                    id           author_id  \\\n",
       "299999  human  t1471504881882075140            17268418   \n",
       "300000  human  t1499064851347304449            33152005   \n",
       "300001  human  t1492208040795324425            76086369   \n",
       "300002  human   t942173606888595457  709501413819293696   \n",
       "300003  human  t1498058873269370880          2218413296   \n",
       "\n",
       "                       created_at  type  \\\n",
       "299999  2021-12-16 15:37:57+00:00  post   \n",
       "300000  2022-03-02 16:51:26+00:00  post   \n",
       "300001  2022-02-11 18:44:55+00:00  post   \n",
       "300002  2017-12-16 23:24:27+00:00  post   \n",
       "300003  2022-02-27 22:14:02+00:00  post   \n",
       "\n",
       "                                                     text language  \\\n",
       "299999  RT @ElemPE1: \"Toys for Tots\"üéÅ\\n\\nLess lecturin...       en   \n",
       "300000                                  @DavidVimes Same.       en   \n",
       "300001          @BigelowLab I spy a @wellsreserve alumna!       en   \n",
       "300002  RT @MPSHaringey: Who thinks #PDBoots deserves ...       en   \n",
       "300003                                 1 BTC = 37,535 USD       vi   \n",
       "\n",
       "       geo_location referenced_tweets_types referenced_tweets_ids  \\\n",
       "299999        False                      {}                    {}   \n",
       "300000        False                      {}                    {}   \n",
       "300001        False                      {}                    {}   \n",
       "300002        False                      {}                    {}   \n",
       "300003        False                      {}                    {}   \n",
       "\n",
       "               user_mentions media_types  in_reply_to_user_id  \\\n",
       "299999  {907432611319914497}          {}                  NaN   \n",
       "300000  {966462024145391616}          {}         9.664620e+17   \n",
       "300001  {260908660,76086369}          {}         2.609087e+08   \n",
       "300002           {407149465}          {}                  NaN   \n",
       "300003                    {}          {}                  NaN   \n",
       "\n",
       "       possibly_sensitive  \n",
       "299999              False  \n",
       "300000              False  \n",
       "300001              False  \n",
       "300002              False  \n",
       "300003              False  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data_sample_nlp.csv\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @EntheosAi: If we show an AI model millions of images and ask it to learn to categorize the world around us, what underlying geometry of‚Ä¶'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"][23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220152"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out tweets that aren't english\n",
    "df = df.loc[df[\"language\"] == \"en\"]\n",
    "df.loc[20:28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "human    202760\n",
       "bot       17392\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts() # unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\stefa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# # Download the stopwords collection from library\n",
    "nltk.download('stopwords')\n",
    "# Put it into a set to guarantee each word only appear once\n",
    "STOPWORDS = list(set(stopwords.words('english')))\n",
    "# Add punctuation to the stopwords list\n",
    "STOPWORDS += [\".\", \"!\", \"?\", \",\", \";\", \":\", \"[\", \"]\", \"{\", \"}\", \"-\", \"+\", \n",
    "    \"_\", \"/\", \"#\", \"$\", \"%\", \"^\", \"&\", \"*\", \"(\", \")\", \"<\", \">\", \"|\", \"=\",\n",
    "    \".-\", \".,\", \"'\", '\"', ',\"', \".>\", \".<\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#sharpiegate', 'poland.‚Ä¶', '#healthinsurance.‚Äù', '#aievents', '#0339', '8-12,', '#tedtalks', 'fastmri.', 'minute-by-minute,', 'mencken', 'chills', '#tips', 'newsblocks', 't&amp;k', 'mobiles', '#thelastofuspart2', '#breakfastlover', 'nova?', '#authorsrock', 'implications.', 'suici‚Ä¶', '\"3x', '#cseiiitd', 'changer.', 'stinger', 'brother#your', 'tamale,', '4(b)', 'kibale', 'bus/lirr/bike/walk/other', '#smallbusinessowner', 'globe.', 'spins!', '#shetland', 'server,', 'tuhina', '‚¨ú‚¨ú‚¨ú‚¨úüü©', 'alleviate.', 'hamiltonian.', '#chayat', 'informative', 'emotion', 'hush', 'spectacle', 'unexplainable', '#nse', '#ool2020', '#worldtravelerer', '#clicklessanalytics4comparison', 'andy', 'nada', 'cecil', 'cvcs', 'fyi', 'lbp/usd.', 'retransitioned', '(aaveusdt)', 'hackney', '#freejazzz', '#gilford', 'narowal', 'üò±üò±üò±üò±', 'online-only', '#machinery', '#instagamer', 'atos', 'hoagie', 'insulins', 'ompc/f', 'farsi', '#consumerprivacy', 'announcement....', 'source)', 'carbene', 'bex!!!', 'üá¨üáß:', 'krita', 'felons', 'reductress', 'jmalvpal:', 'bcl', 'cny.', 'silky', 'enhancements.', 'nanyuki', '~2016', 'aran', '‚õîÔ∏è‚ÄºÔ∏èüî¥]', \"byfield's\", 'essence!', 'kenna', 'anti-vax,', 'grant‚Äîand', 'gathering', '#fusible', 'tarzana', 'incubator', 'abdulaev', 'somatic', 'dasgupta‚Äôs']\n",
      "\n",
      "Total unique words in all tweets: 336907\n",
      "\n",
      "['sharpiegate', 'poland', 'healthinsurance', 'aievents', '0339', '812', 'tedtalks', 'fastmri', 'minutebyminute', 'mencken', 'chills', 'tips', 'newsblocks', 'tampk', 'mobiles', 'thelastofuspart2', 'breakfastlover', 'nova', 'authorsrock', 'implications', 'suici', '3x', 'cseiiitd', 'changer', 'stinger', 'brotheryour', 'tamale', '4b', 'kibale', 'buslirrbikewalkother', 'smallbusinessowner', 'globe', 'spins', 'shetland', 'server', 'tuhina', 'alleviate', 'hamiltonian', 'chayat', 'informative', 'emotion', 'hush', 'spectacle', 'unexplainable', 'nse', 'ool2020', 'worldtravelerer', 'clicklessanalytics4comparison', 'andy', 'nada', 'cecil', 'cvcs', 'fyi', 'lbpusd', 'retransitioned', 'aaveusdt', 'hackney', 'freejazzz', 'gilford', 'narowal', 'onlineonly', 'machinery', 'instagamer', 'atos', 'hoagie', 'insulins', 'ompcf', 'farsi', 'consumerprivacy', 'announcement', 'source', 'carbene', 'bex', 'krita', 'felons', 'reductress', 'jmalvpal', 'bcl', 'cny', 'silky', 'enhancements', 'nanyuki', '2016', 'aran', 'byfields', 'essence', 'kenna', 'antivax', 'grantand', 'gathering', 'fusible', 'tarzana', 'incubator', 'abdulaev', 'somatic', 'dasguptas', 'marlboro', '36days_c', 'conncoll', 'anticas']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Questions : \n",
    "1. In general, what are the characteristics of a text written by a social bot?\n",
    "2. Do they not contain account tags? \n",
    "3. How do we handle emojies? Do they indicate that an account is a bot?\n",
    "4. Do we only stick to English?\n",
    "\n",
    "Notes: \n",
    "The number of unique words are still too high?\n",
    "Very slow when remove emojies\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def preprocess(df):\n",
    "    \"\"\"\n",
    "    This function takes a dataframe with different features \n",
    "    and returns pre-processed texts from the tweets \n",
    "    \"\"\"\n",
    "\n",
    "    # update tweets by lowercase, strip and tokenize\n",
    "    unique_word_freqs = set()\n",
    "    df['text'].str.lower().str.split().apply(unique_word_freqs.update)\n",
    "\n",
    "    # exclude stop words, tagged accounts, punctuation, links and numbers    \n",
    "    unique_word_freqs = list(unique_word_freqs)\n",
    "    data = [word for i,word in enumerate(unique_word_freqs) if ( '@' not in word) and ('http' not in word) \\\n",
    "            and (word not in STOPWORDS) and (not word.isnumeric())]\n",
    "    print(data[:100])\n",
    "    \n",
    "    # remove emojies\n",
    "    clean_text = \" \".join([word for word in data if not any(i in word for i in emoji.EMOJI_DATA)])\n",
    "    \n",
    "    #remove punctuation inside of words\n",
    "    clean_text = re.sub(r'[^\\w\\s]', '', clean_text).split()\n",
    "\n",
    "    \n",
    "    print(f'\\nTotal unique words in all tweets: {len(clean_text)}\\n')   \n",
    "    return clean_text\n",
    "    \n",
    "data = preprocess(df)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rep zng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier():\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "917465e231de0303214f3286e28ee0d1023542557ef1c3f474efb076deb95ba9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
